{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09c2cbd",
   "metadata": {},
   "source": [
    "# Generative Models in JAX Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d6f18",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "\n",
    "* GANs\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5dc633",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks:\n",
    "\n",
    "Model architectures Implemented:\n",
    "* Vanilla GAN\n",
    "* DC GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3c50e",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fa411",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01906e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d0b8b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ec2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, static_broadcasted_argnums=(1, 2))\n",
    "def create_state(rng, model_cls, input_shape): \n",
    "  r\"\"\"Create the training state given a model class. \"\"\" \n",
    "  model = model_cls()\n",
    "\n",
    "  tx = optax.adam(0.0002, b1=0.5, b2=0.999)\n",
    "  variables = model.init(rng, jnp.ones(input_shape))\n",
    "\n",
    "  state = TrainState.create(apply_fn=model.apply, tx=tx, \n",
    "      params=variables['params'], batch_stats=variables['batch_stats'])\n",
    "  \n",
    "  return state\n",
    "\n",
    "\n",
    "@jax.pmap\n",
    "def sample_from_generator(generator_state, input_noise):\n",
    "  \"\"\"Sample from the generator in evaluation mode.\"\"\"\n",
    "  generated_data = generator_state.apply_fn(\n",
    "      {'params': generator_state.params,\n",
    "       'batch_stats': generator_state.batch_stats},\n",
    "      input_noise, train=False, mutable=False)\n",
    "  return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ee5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc40fc7",
   "metadata": {},
   "source": [
    "# Vanilla GAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_gan(nn.module):\n",
    "    super().__init__()\n",
    "    def __init__(self):\n",
    "        \n",
    "    def __call__():\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "class Generator(nn.module):\n",
    "    features: int = 64\n",
    "    dtype: type = jnp.float32\n",
    "        \n",
    "    @nncompact\n",
    "    def __call__():\n",
    "        conv_transpose = partial(nn.ConvTranspose, padding='VALID',\n",
    "                             kernel_init=normal_init(0.02), dtype=self.dtype)\n",
    "        batch_norm = partial(nn.BatchNorm, use_running_average=not train, axis=-1, \n",
    "                             scale_init=normal_init(0.02), dtype=self.dtype)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    class Generator(nn.Module):\n",
    "  features: int = 64\n",
    "  dtype: type = jnp.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(hk.Module):\n",
    "  \"\"\"Generator network.\"\"\"\n",
    "\n",
    "  def __init__(self, output_channels=(32, 1), name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.output_channels = output_channels\n",
    "\n",
    "  def __call__(self, x):\n",
    "    \"\"\"Maps noise latents to images.\"\"\"\n",
    "    x = hk.Linear(7 * 7 * 64)(x)\n",
    "    x = jnp.reshape(x, x.shape[:1] + (7, 7, 64))\n",
    "    for output_channels in self.output_channels:\n",
    "      x = jax.nn.relu(x)\n",
    "      x = hk.Conv2DTranspose(output_channels=output_channels,\n",
    "                             kernel_shape=[5, 5],\n",
    "                             stride=2,\n",
    "                             padding=\"SAME\")(x)\n",
    "    # We use a tanh to ensure that the generated samples are in the same\n",
    "    # range as the data.\n",
    "    return jnp.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(hk.Module):\n",
    "  \"\"\"Discriminator network.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               output_channels=(8, 16, 32, 64, 128),\n",
    "               strides=(2, 1, 2, 1, 2),\n",
    "               name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.output_channels = output_channels\n",
    "    self.strides = strides\n",
    "\n",
    "  def __call__(self, x):\n",
    "    \"\"\"Classifies images as real or fake.\"\"\"\n",
    "    for output_channels, stride in zip(self.output_channels, self.strides):\n",
    "      x = hk.Conv2D(output_channels=output_channels,\n",
    "                    kernel_shape=[5, 5],\n",
    "                    stride=stride,\n",
    "                    padding=\"SAME\")(x)\n",
    "      x = jax.nn.leaky_relu(x, negative_slope=0.2)\n",
    "    x = hk.Flatten()(x)\n",
    "    # We have two classes: 0 = input is fake, 1 = input is real.\n",
    "    logits = hk.Linear(2)(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "  \"\"\"A basic GAN.\"\"\"\n",
    "\n",
    "  def __init__(self, num_latents):\n",
    "    self.num_latents = num_latents\n",
    "\n",
    "    # Define the Haiku network transforms.\n",
    "    # We don't use BatchNorm so we don't use `with_state`.\n",
    "    self.gen_transform = hk.without_apply_rng(\n",
    "        hk.transform(lambda *args: Generator()(*args)))\n",
    "    self.disc_transform = hk.without_apply_rng(\n",
    "        hk.transform(lambda *args: Discriminator()(*args)))\n",
    "\n",
    "    # Build the optimizers.\n",
    "    self.optimizers = GANTuple(gen=optax.adam(1e-4, b1=0.5, b2=0.9),\n",
    "                               disc=optax.adam(1e-4, b1=0.5, b2=0.9))\n",
    "\n",
    "  @functools.partial(jax.jit, static_argnums=0)\n",
    "  def initial_state(self, rng, batch):\n",
    "    \"\"\"Returns the initial parameters and optimize states.\"\"\"\n",
    "    # Generate dummy latents for the generator.\n",
    "    dummy_latents = jnp.zeros((batch.shape[0], self.num_latents))\n",
    "\n",
    "    # Get initial network parameters.\n",
    "    rng_gen, rng_disc = jax.random.split(rng)\n",
    "    params = GANTuple(gen=self.gen_transform.init(rng_gen, dummy_latents),\n",
    "                      disc=self.disc_transform.init(rng_disc, batch))\n",
    "    print(\"Generator: \\n\\n{}\\n\".format(tree_shape(params.gen)))\n",
    "    print(\"Discriminator: \\n\\n{}\\n\".format(tree_shape(params.disc)))\n",
    "\n",
    "    # Initialize the optimizers.\n",
    "    opt_state = GANTuple(gen=self.optimizers.gen.init(params.gen),\n",
    "                         disc=self.optimizers.disc.init(params.disc))\n",
    "    return GANState(params=params, opt_state=opt_state)\n",
    "\n",
    "\n",
    "    def sample(self, rng, gen_params, num_samples):\n",
    "    \"\"\"Generates images from noise latents.\"\"\"\n",
    "    latents = jax.random.normal(rng, shape=(num_samples, self.num_latents))\n",
    "    return self.gen_transform.apply(gen_params, latents)\n",
    "\n",
    "  def gen_loss(self, gen_params, rng, disc_params, batch):\n",
    "    \"\"\"Generator loss.\"\"\"\n",
    "    # Sample from the generator.\n",
    "    fake_batch = self.sample(rng, gen_params, num_samples=batch.shape[0])\n",
    "\n",
    "    # Evaluate using the discriminator. Recall class 1 is real.\n",
    "    fake_logits = self.disc_transform.apply(disc_params, fake_batch)\n",
    "    fake_probs = jax.nn.softmax(fake_logits)[:, 1]\n",
    "    loss = -jnp.log(fake_probs)\n",
    "\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "  def disc_loss(self, disc_params, rng, gen_params, batch):\n",
    "    \"\"\"Discriminator loss.\"\"\"\n",
    "    # Sample from the generator.\n",
    "    fake_batch = self.sample(rng, gen_params, num_samples=batch.shape[0])\n",
    "\n",
    "    # For efficiency we process both the real and fake data in one pass.\n",
    "    real_and_fake_batch = jnp.concatenate([batch, fake_batch], axis=0)\n",
    "    real_and_fake_logits = self.disc_transform.apply(disc_params,\n",
    "                                                     real_and_fake_batch)\n",
    "    real_logits, fake_logits = jnp.split(real_and_fake_logits, 2, axis=0)\n",
    "\n",
    "    # Class 1 is real.\n",
    "    real_labels = jnp.ones((batch.shape[0],), dtype=jnp.int32)\n",
    "    real_loss = sparse_softmax_cross_entropy(real_logits, real_labels)\n",
    "\n",
    "    # Class 0 is fake.\n",
    "     fake_labels = jnp.zeros((batch.shape[0],), dtype=jnp.int32)\n",
    "    fake_loss = sparse_softmax_cross_entropy(fake_logits, fake_labels)\n",
    "\n",
    "    return jnp.mean(real_loss + fake_loss)\n",
    "\n",
    "  @functools.partial(jax.jit, static_argnums=0)\n",
    "  def update(self, rng, gan_state, batch):\n",
    "    \"\"\"Performs a parameter update.\"\"\"\n",
    "    rng, rng_gen, rng_disc = jax.random.split(rng, 3)\n",
    "\n",
    "    # Update the discriminator.\n",
    "    disc_loss, disc_grads = jax.value_and_grad(self.disc_loss)(\n",
    "        gan_state.params.disc,\n",
    "        rng_disc,\n",
    "        gan_state.params.gen,\n",
    "        batch)\n",
    "    disc_update, disc_opt_state = self.optimizers.disc.update(\n",
    "        disc_grads, gan_state.opt_state.disc)\n",
    "    disc_params = optax.apply_updates(gan_state.params.disc, disc_update)\n",
    "\n",
    "    # Update the generator.\n",
    "    gen_loss, gen_grads = jax.value_and_grad(self.gen_loss)(\n",
    "        gan_state.params.gen,\n",
    "        rng_gen,\n",
    "        gan_state.params.disc,\n",
    "        batch)\n",
    "    gen_update, gen_opt_state = self.optimizers.gen.update(\n",
    "        gen_grads, gan_state.opt_state.gen)\n",
    "    gen_params = optax.apply_updates(gan_state.params.gen, gen_update)\n",
    "    params = GANTuple(gen=gen_params, disc=disc_params)\n",
    "    opt_state = GANTuple(gen=gen_opt_state, disc=disc_opt_state)\n",
    "    gan_state = GANState(params=params, opt_state=opt_state)\n",
    "    log = {\n",
    "        \"gen_loss\": gen_loss,\n",
    "        \"disc_loss\": disc_loss,\n",
    "    }\n",
    "\n",
    "    return rng, gan_state, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9d3bb",
   "metadata": {},
   "source": [
    "# Traning GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153c09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd29d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
